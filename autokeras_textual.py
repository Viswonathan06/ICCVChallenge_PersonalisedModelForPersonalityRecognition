# -*- coding: utf-8 -*-
"""AutoKeras_textual.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g3rqot6Kg98RkInZA1iOWHHrW7ZA5-e1
"""

import autokeras as ak
import tensorflow as tf

# import keras
import os
import h5py
import numpy as np
import pickle
import pandas as pd
from functools import reduce

"""## Upload the validation and training metadata"""

# VALIDATION DATA
path = "/mnt/CSV files/Final_val.csv"
print(path)
#df = pd.read_pickle(path)
metadata_valid = pd.read_csv(path)
#metadata_valid.drop(['Unnamed: 0','TIMESTAMP','COUNTRY','EDUCATION','LANGUAGE'],inplace=True,axis=1)
metadata_valid.sort_values(['Video'],inplace=True)

metadata_valid.head()
# metadata_valid['LANGUAGE'].uniqueue()


# TRAINING DATA
path = "/mnt/CSV files/Final-3.csv"
print(path)

# Reading training file.
metadata_train=pd.read_csv(path)
metadata_train.drop(['Unnamed: 0','LANGUAGE'],inplace=True,axis=1)
metadata_train.sort_values(['Video'],inplace=True)
metadata_train

# TESTING DATA
path = "/mnt/CSV files/final_test.csv"
print(path)

# Reading training file.
metadata_test=pd.read_csv(path)
metadata_test.drop(['Unnamed: 0'],inplace=True,axis=1)
metadata_test.sort_values(['Video'],inplace=True)
metadata_test

"""## Upload the training, validation and test data"""

path_data = "/mnt/Features"
path_text_features = path_data+"/TextualFeatures/"
path_audio_features = path_data+"/AudioFeatures/"


path_text_features_train = path_text_features+"Train/"
path_text_features_valid = path_text_features+"Validation/"
path_text_features_test = path_text_features+"Test/"

import glob

def import_data(path_features):
    
    listing=os.listdir(path_features)

    df_sentiment = pd.DataFrame(columns = ["video","context","person","minute","SentiMin", "SentiMax", "SentiAvg", "SentiStdev", "Senti0-0.20", "Senti0.21-0.40", "Senti0.41-0.60", "Senti0.61-0.80", "Senti0.81-1.00", "SentiTotal"])
    df_speechtrait = pd.DataFrame(columns =["video","context","person","minute","TraitMin","TraitMax","TraitAvg","TraitStdev","TraitTotal"])
    df_talkturn = pd.DataFrame(columns =["video","context","person","minute","TurnPercent","AvgWrdPerTurn","LongestTurn","TotalWrds","STDWrdPerTurn"])
    df_time = pd.DataFrame(columns=["video","context","person","minute","TimeMin","TimeMax","TimeAvg","TimeStdev","TimeTotal"])

    i = 0
    for folder in listing:
        if folder == ".DS_Store":
            continue
        else:
            print(folder)
            file_listing = os.listdir(path_features+folder+"/")
            for file in file_listing:
                print(file)
                fname =glob.glob(path_features+folder+'/'+file+'/output2/*.csv')
                #print(fname)
                for fn in fname:
                    print(fn)
                    df_fn = pd.read_csv(fn,index_col=None)
                    filename = fn.split('/')[8].split('.')[0]
                    print(filename)
                    #'127184_lego_P2_10_Time'
                    feature_name = filename.split('_')[4]
                    if feature_name == "RawSenti":
                        df_sentiment.at[i,"video"]=filename.split('_')[0]
                        df_sentiment.at[i,"context"]=filename.split('_')[1]
                        df_sentiment.at[i,"person"]=filename.split('_')[2]
                        df_sentiment.at[i,"minute"]=filename.split('_')[3]
                        df_sentiment.at[i,4:]=df_fn.values
                    elif feature_name == "SpeechTrait":
                        df_speechtrait.at[i,"video"]=filename.split('_')[0]
                        df_speechtrait.at[i,"context"]=filename.split('_')[1]
                        df_speechtrait.at[i,"person"]=filename.split('_')[2]
                        df_speechtrait.at[i,"minute"]=filename.split('_')[3]
                        df_speechtrait.at[i,4:]=df_fn.values
                    elif feature_name == "TalkTurn":
                        df_talkturn.at[i,"video"]=filename.split('_')[0]
                        df_talkturn.at[i,"context"]=filename.split('_')[1]
                        df_talkturn.at[i,"person"]=filename.split('_')[2]
                        df_talkturn.at[i,"minute"]=filename.split('_')[3]
                        df_talkturn.at[i,4:]=df_fn.values
                    elif feature_name == "Time":
                        df_time.at[i,"video"]=filename.split('_')[0]
                        df_time.at[i,"context"]=filename.split('_')[1]
                        df_time.at[i,"person"]=filename.split('_')[2]
                        df_time.at[i,"minute"]=filename.split('_')[3]
                        df_time.at[i,4:]=df_fn.values
                    i = i+1
                    #print("========")
    df_sentiment['video']=df_sentiment['video'].astype(float)
    df_speechtrait['video']=df_speechtrait['video'].astype(float)
    df_talkturn['video']=df_talkturn['video'].astype(float)
    df_time['video']=df_time['video'].astype(float)
    df_sentiment.sort_values(['video', 'context','person','minute'],inplace=True)
    df_speechtrait.sort_values(['video', 'context','person','minute'],inplace=True)
    df_talkturn.sort_values(['video', 'context','person','minute'],inplace=True)
    df_time.sort_values(['video', 'context','person','minute'],inplace=True)
    df_textualall=pd.concat([df_sentiment.reset_index(drop=True),df_speechtrait.reset_index(drop=True),df_talkturn.reset_index(drop=True),df_time.reset_index(drop=True)], axis=1)
    df_textualall = df_textualall.loc[:,~df_textualall.columns.duplicated()]
    df_sentiment.dropna(inplace=True)
    df_speechtrait.dropna(inplace=True)
    df_talkturn.dropna(inplace=True)
    df_time.dropna(inplace=True)
    df_textualall.dropna(inplace=True)
    df_sentiment[['Senti0-0.20', 'Senti0.21-0.40','Senti0.41-0.60', 'Senti0.61-0.80', 'Senti0.81-1.00']]= df_sentiment[['Senti0-0.20', 'Senti0.21-0.40','Senti0.41-0.60', 'Senti0.61-0.80', 'Senti0.81-1.00']].div(df_sentiment[['Senti0-0.20', 'Senti0.21-0.40','Senti0.41-0.60', 'Senti0.61-0.80', 'Senti0.81-1.00']].sum(axis=1), axis=0)
    return df_sentiment, df_speechtrait, df_talkturn, df_time,df_textualall

df_sentiment_train, df_speechtrait_train, df_talkturn_train, df_time_train,df_textualall_train = import_data(path_text_features_train)

df_sentiment_valid, df_speechtrait_valid, df_talkturn_valid, df_time_valid,df_textualall_valid = import_data(path_text_features_valid)

df_sentiment_test, df_speechtrait_test, df_talkturn_test, df_time_test,df_textualall_test = import_data(path_text_features_test)

def merge_with_metadata(df,metadata,HOW):
    #merge with the metadata on the video identity
    df_=df.merge(metadata,left_on="video",right_on="Video", how=HOW)
    #df_ = df_.loc[:,['video','person','minute','Video','PART.1','PART.2','ID_y']]
    #convert the ID_y to float to be able to compare it with PART.1 and PART.2
    df_['ID_y']=df_['ID_y'].astype(float)
    #keep only the lines where PART.1 and P1 match or P2 and PART.2 match
    c1 = ((df_['PART.1']==df_["ID_y"]) & (df_["person"] == "P1"))
    c2 = ((df_['PART.2']==df_["ID_y"]) & (df_["person"] == "P2"))
    c = c1 | c2
    df_ = df_[c]
    df_.drop_duplicates(subset =["video","context","person","minute"],inplace = True)
    #Remove unnecessary columns
    df_.drop(['SESSION1', 'SESSION2','SESSION3','SESSION4','Video','video','PART.1','PART.2','person'], axis = 1,inplace=True)
    return df_

"""## Create different datasets according to the profile"""

def create_profile_dataset(df,column):
    if (column=='GENDER'):
        df_m=df[df['GENDER']=='M']
        df_f=df[df['GENDER']=='F']
        df_m_meta = df_m[['ID_y','minute']].copy()
        df_f_meta = df_f[['ID_y','minute']].copy()
        df_m.drop(['GENDER','context','AGE','ID_y','minute'], axis = 1,inplace=True)
        df_f.drop(['GENDER','context','AGE','ID_y','minute'], axis = 1,inplace=True)
        #df.drop(['context','AGE'], axis = 1,inplace=True)
        return df_m,df_f,df_m_meta,df_f_meta
    elif (column=='AGE'):
        df.drop(['context','GENDER'], axis = 1,inplace=True)
        df_ag1 = df[df['AGE'] <= 20]
        df_ag2 = df[(df['AGE'] > 20) & (df['AGE'] <=40)]
        df_ag3 = df[df['AGE'] > 40]
        df_ag1.drop(['AGE'], axis = 1,inplace=True)
        df_ag2.drop(['AGE'], axis = 1,inplace=True)
        df_ag3.drop(['AGE'], axis = 1,inplace=True)
        return df_ag1,df_ag2,df_ag3
    elif (column=='LANGUAGE'):
        df.drop(['GENDER','context','AGE'], axis = 1,inplace=True)
        lang = df['LANGUAGE'].unique()
        df_sp = df[df['LANGUAGE']== lang[0]]
        df_cat = df[df['LANGUAGE']==lang[1]]
        df_eng = df[df['LANGUAGE']==lang[2]]
        df_sp.drop(['LANGUAGE'], axis = 1,inplace=True)
        df_cat.drop(['LANGUAGE'], axis = 1,inplace=True)
        df_eng.drop(['LANGUAGE'], axis = 1,inplace=True)
        return df_sp,df_cat,df_eng
    elif (column=='context'):
        df.drop(['GENDER','AGE'], axis = 1,inplace=True)
        context = df['context'].unique()
        df_lego = df[df['context']== context[0]]
        df_animals = df[df['context']==context[1]]
        df_talk = df[df['context']==context[2]]
        df_ghost = df[df['context']==context[3]]
        df_lego.drop(['context'], axis = 1,inplace=True)
        df_animals.drop(['context'], axis = 1,inplace=True)
        df_talk.drop(['context'], axis = 1,inplace=True)
        df_ghost.drop(['context'], axis = 1,inplace=True)
        return df_lego,df_animals,df_talk,df_ghost

import csv
def prepare_data_for_ML(df,dataset):
    if dataset=='test':
        X = df.to_numpy()
        X = np.asarray(X).astype(np.float32)
        return X
    else:
        y_o = df.iloc[:,-5].to_numpy()
        y_c = df.iloc[:,-4].to_numpy()
        y_e = df.iloc[:,-3].to_numpy()
        y_a = df.iloc[:,-2].to_numpy()
        y_n = df.iloc[:,-1].to_numpy()
        ocean = df.iloc[:,-5:]
        X   = df.iloc[:,:-5].to_numpy()
        X = np.asarray(X).astype(np.float32)
        o_set = tf.data.Dataset.from_tensor_slices((X, y_o))
        c_set = tf.data.Dataset.from_tensor_slices((X, y_c))
        e_set = tf.data.Dataset.from_tensor_slices((X, y_e))
        a_set = tf.data.Dataset.from_tensor_slices((X, y_a))
        n_set = tf.data.Dataset.from_tensor_slices((X, y_n))
        return o_set,c_set,e_set,a_set,n_set,ocean

df_sentiment_valid_merged = merge_with_metadata(df_sentiment_valid,metadata_valid,"left")
df_speechtrait_valid_merged = merge_with_metadata(df_speechtrait_valid,metadata_valid,"left")
df_talkturn_valid_merged = merge_with_metadata(df_talkturn_valid,metadata_valid,"left")
df_time_valid_merged = merge_with_metadata(df_time_valid,metadata_valid,"left")
df_alltextual_valid_merged = merge_with_metadata(df_textualall_valid,metadata_valid,"left")



df_sentiment_train_merged = merge_with_metadata(df_sentiment_train,metadata_train,"left")
df_speechtrait_train_merged = merge_with_metadata(df_speechtrait_train,metadata_train,"left")
df_talkturn_train_merged = merge_with_metadata(df_talkturn_train,metadata_train,"left")
df_time_train_merged = merge_with_metadata(df_time_train,metadata_train,"left")
df_alltextual_train_merged = merge_with_metadata(df_textualall_train,metadata_train,"left")




df_sentiment_test_merged = merge_with_metadata(df_sentiment_test,metadata_test,"left")
df_speechtrait_test_merged = merge_with_metadata(df_speechtrait_test,metadata_test,"left")
df_talkturn_test_merged = merge_with_metadata(df_talkturn_test,metadata_test,"left")
df_time_test_merged = merge_with_metadata(df_time_test,metadata_test,"left")
df_alltextual_test_merged = merge_with_metadata(df_textualall_test,metadata_test,"left")

for iter_feature in ['talkturn','sentiment','time','speechtrait','alltextual']:
    for iter_set in ['valid','train','test']:
        print(f"Feature: {iter_feature}, Set: {iter_set}")
        exec(f"df_{iter_feature}_{iter_set}_male,df_{iter_feature}_{iter_set}_female,df_{iter_feature}_{iter_set}_male_meta,df_{iter_feature}_{iter_set}_female_meta=create_profile_dataset(df_{iter_feature}_{iter_set}_merged,'GENDER')")
        for iter_gender in ['male','female']:
            if iter_set=='test':
                exec(f"{iter_gender}_{iter_set}_{iter_feature}_X=prepare_data_for_ML(df_{iter_feature}_{iter_set}_{iter_gender},iter_set)")
            else:
                exec(f"{iter_gender}_{iter_set}_{iter_feature}_o,{iter_gender}_{iter_set}_{iter_feature}_c,{iter_gender}_{iter_set}_{iter_feature}_e,{iter_gender}_{iter_set}_{iter_feature}_a,{iter_gender}_{iter_set}_{iter_feature}_n,ocean_{iter_gender}_{iter_set}=prepare_data_for_ML(df_{iter_feature}_{iter_set}_{iter_gender},iter_set)")
 
            
    
#male_train_set_o,male_train_set_c,male_train_set_e,male_train_set_a,male_train_set_n = prepare_data_for_ML(df_talkturn_train_m)
#male_train_set_o,male_train_set_c,male_train_set_e,male_train_set_a,male_train_set_n = prepare_data_for_ML(df_talkturn_train_m)
#female_train_set_o,female_train_set_c,female_train_set_e,female_train_set_a,female_train_set_n = prepare_data_for_ML(df_talkturn_train_f)
#male_val_set_o,male_val_set_c,male_val_set_e,male_val_set_a,male_val_set_n = prepare_data_for_ML(df_talkturn_valid_m)
#female_val_set_o,female_val_set_c,female_val_set_e,female_val_set_a,female_val_set_n = prepare_data_for_ML(df_talkturn_valid_f)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger

model_dir='/mnt/Models'
results_dir = "/mnt/Results"
df_validation_loss =  pd.DataFrame(columns = ["Pers_Label","Feature","Profile_Split","val_loss"])
#df_predicted_val= pd.DataFrame(columns = ['o','c','e','a','n'])
#df_predicted_test= pd.DataFrame(columns = ['o','c','e','a','n'])


try:
    os.mkdir(results_dir)
except OSError:
    print ("Creation of the directory %s failed" % results_dir)
else:
    print ("Successfully created the directory %s " % results_dir)
    
try:
    os.mkdir(model_dir)
except OSError:
    print ("Creation of the directory %s failed" % model_dir)
else:
    print ("Successfully created the directory %s " % model_dir)    
    
#set early stopping criteria
#pat = 100 #this is the number of epochs with no improvment after which the training will stop
#early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)
nb_trial = 100
nb_epochs = 1000

def fit_and_evaluate(tf_train_set,tf_val_set,X_test,TRIALS,EPOCHS,model_filename,csv_logger):
    fit_results=None
    model_reg = None
    model_reg = ak.StructuredDataRegressor(max_trials=TRIALS, overwrite=True)
    fit_results = model_reg.fit(tf_train_set, epochs=EPOCHS, validation_split=0.15,callbacks=[csv_logger])#validation_data=tf_val_set
    validation_loss = model_reg.evaluate(tf_val_set)
    predicted_y_val = model_reg.predict(tf_val_set)
    predicted_y_test = model_reg.predict(X_test)
    model = model_reg.export_model()
    print(type(model))  # <class 'tensorflow.python.keras.engine.training.Model'>
    try:
        model.save(model_filename, save_format="tf")
    except Exception:
        model.save(model_filename+".h5")
    del model_reg
    del model
    return fit_results,validation_loss,predicted_y_val,predicted_y_test

i = 0

for iter_gender in ['male','female']:
    for iter_feature in ['speechtrait','talkturn','sentiment','time']:#'alltextual'
        for iter_pers_label in ['o','c','e','a','n']:
            print(f"Personality Label: {iter_pers_label}, Gender: {iter_gender}, Feature: {iter_feature}")
            logfilename = results_dir+f"/fit_results_{iter_feature}_{iter_pers_label}_{iter_gender}.csv"
            #model_checkpoint = ModelCheckpoint('./Models/TalkTurn_'+str(iter_gender)+'_'+str(iter_pers_label)+'.tf', verbose=1, save_best_only=True,save_format='tf')
            model_filename = f"/mnt/Models/{iter_feature}_{iter_gender}_{iter_pers_label}"
            csv_logger = CSVLogger(logfilename, append=True, separator=';')
            exec(f"fit_results_{iter_feature}_{iter_pers_label}_{iter_gender},val_loss_{iter_feature}_{iter_pers_label}_{iter_gender},predicted_val_{iter_feature}_{iter_pers_label}_{iter_gender},predicted_test_{iter_feature}_{iter_pers_label}_{iter_gender}=fit_and_evaluate({iter_gender}_train_{iter_feature}_{iter_pers_label},{iter_gender}_valid_{iter_feature}_{iter_pers_label},{iter_gender}_test_{iter_feature}_X,nb_trial,nb_epochs,model_filename,csv_logger)")
            exec(f"df_validation_loss.at[i,'val_loss']=val_loss_{iter_feature}_{iter_pers_label}_{iter_gender}")
            exec(f"df_validation_loss.at[i,'Pers_Label']=iter_pers_label")
            exec(f"df_validation_loss.at[i,'Feature']=iter_feature")
            exec(f"df_validation_loss.at[i,'Profile_Split']=iter_gender")
            i = i+1
            if (iter_pers_label=='o'):
                exec(f"df_predicted_val_{iter_feature}_{iter_gender} = pd.DataFrame(data=predicted_val_{iter_feature}_{iter_pers_label}_{iter_gender},columns=['o'])") 
                exec(f"df_predicted_test_{iter_feature}_{iter_gender} = pd.DataFrame(data=predicted_test_{iter_feature}_{iter_pers_label}_{iter_gender},columns=['o'])") 
            else:
                exec(f"df_predicted_val_{iter_feature}_{iter_gender}[iter_pers_label]=predicted_val_{iter_feature}_{iter_pers_label}_{iter_gender}")
                exec(f"df_predicted_test_{iter_feature}_{iter_gender}[iter_pers_label]=predicted_test_{iter_feature}_{iter_pers_label}_{iter_gender}")
            
        exec(f"df_predicted_val_{iter_feature}_{iter_gender}.to_csv('{results_dir}/{iter_feature}_{iter_gender}_val.csv',index=False)")
        exec(f"df_predicted_test_{iter_feature}_{iter_gender}.to_csv('{results_dir}/{iter_feature}_{iter_gender}_test.csv',index=False)")

        
df_validation_loss.to_csv(results_dir+"/ValidationLoss.csv",index=False)

male_valid_talkturn_o

from tensorflow.keras.models import load_model
loaded_model = load_model("/mnt/Models/talkturn_male_o", custom_objects=ak.CUSTOM_OBJECTS)
loaded_model#.evaluate(male_train_talkturn_o)
#model_reg.evaluate(tf_val_set)

"""## Decision Fusion
Read the predictions for each modality for each personality label.

"""

df_talkturn_valid_male_meta.head()

ocean_male_valid.head()

df_predicted_val_talkturn_male.head()

frames = [df_talkturn_valid_male_meta.reset_index(drop=True), df_predicted_val_talkturn_male.reset_index(drop=True), ocean_male_valid.reset_index(drop=True)]

result = pd.concat(frames, axis=1)

result

for iter_gender in ['male','female']:
    for iter_feature in ['speechtrait','talkturn','sentiment','time']:
        print(f" Gender: {iter_gender}, Feature: {iter_feature}")  
        exec(f"df_predicted_valid_{iter_feature}_{iter_gender}=pd.read_csv('{results_dir}/{iter_feature}_{iter_gender}_val.csv')")
        exec(f"df_predicted_test_{iter_feature}_{iter_gender}=pd.read_csv('{results_dir}/{iter_feature}_{iter_gender}_test.csv')")

df_o_male=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','o'])
df_o_female=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','o'])

df_c_male=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','c'])
df_c_female=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','c'])

df_e_male=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','e'])
df_e_female=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','e'])

df_a_male=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','a'])
df_a_female=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','a'])


df_n_male=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','n'])
df_n_female=pd.DataFrame(columns=['talkturn','sentiment','speechtrait','time','n'])


for iter_gender in ['male','female']:
    for iter_pers_label in ['o','c','e','a','n']:
        for iter_set in ['valid']:
            exec(f"df_{iter_pers_label}_{iter_gender}['talkturn'] = df_predicted_{iter_set}_talkturn_{iter_gender}[iter_pers_label].reset_index(drop=True).copy()")
            exec(f"df_{iter_pers_label}_{iter_gender}['sentiment'] = df_predicted_{iter_set}_sentiment_{iter_gender}[iter_pers_label].reset_index(drop=True).copy()")
            exec(f"df_{iter_pers_label}_{iter_gender}['speechtrait'] = df_predicted_{iter_set}_speechtrait_{iter_gender}[iter_pers_label].reset_index(drop=True).copy()")
            exec(f"df_{iter_pers_label}_{iter_gender}['time'] = df_predicted_{iter_set}_time_{iter_gender}[iter_pers_label].reset_index(drop=True).copy()")
            if iter_pers_label=="o":
                exec(f"df_{iter_pers_label}_{iter_gender}[iter_pers_label] = ocean_{iter_gender}_{iter_set}['OPENMINDEDNESS_Z'].reset_index(drop=True).copy()")
            elif iter_pers_label=="c":
                exec(f"df_{iter_pers_label}_{iter_gender}[iter_pers_label] = ocean_{iter_gender}_{iter_set}['CONSCIENTIOUSNESS_Z'].reset_index(drop=True).copy()")
            elif iter_pers_label=="e":
                exec(f"df_{iter_pers_label}_{iter_gender}[iter_pers_label] = ocean_{iter_gender}_{iter_set}['EXTRAVERSION_Z'].reset_index(drop=True).copy()")
            elif iter_pers_label=="a":    
                exec(f"df_{iter_pers_label}_{iter_gender}[iter_pers_label] = ocean_{iter_gender}_{iter_set}['AGREEABLENESS_Z'].reset_index(drop=True).copy()")
            elif iter_pers_label=="n":    
                exec(f"df_{iter_pers_label}_{iter_gender}[iter_pers_label] = ocean_{iter_gender}_{iter_set}['NEGATIVEEMOTIONALITY_Z'].reset_index(drop=True).copy()")

df_o_female.iloc[:,4]

# from sklearn.linear_model import Ridge
# from sklearn import metrics


# import numpy as np

# clf = Ridge(alpha=1.0)
# clf.fit(df_o_female.iloc[:,0:4], df_o_female.iloc[:,4])
# Ridge()

# clf.score(df_o_female.iloc[:,0:4], df_o_female.iloc[:,4])

# clf.predict(df_o_female.iloc[:,0:4])

df_talkturn_test_male_meta.to_csv('/mnt/Results/test_male_meta.csv')
df_talkturn_test_female_meta.to_csv('/mnt/Results/test_female_meta.csv')

