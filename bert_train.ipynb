{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 19:22:02.975444: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-08 19:22:02.975466: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "import autokeras as ak\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files. \n",
    "data_train = pd.read_csv('./Features/BERT/bert_train.csv')\n",
    "data_validation = pd.read_csv('./Features/BERT/bert_validation.csv')\n",
    "data_test = pd.read_csv('./Features/BERT/bert_test.csv')\n",
    "\n",
    "\"\"\"# Data Handling\"\"\"\n",
    "data_train.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "data_validation.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "data_test.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_y</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>OPENMINDEDNESS_Z</th>\n",
       "      <th>CONSCIENTIOUSNESS_Z</th>\n",
       "      <th>EXTRAVERSION_Z</th>\n",
       "      <th>AGREEABLENESS_Z</th>\n",
       "      <th>NEGATIVEEMOTIONALITY_Z</th>\n",
       "      <th>minutes</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>42</td>\n",
       "      <td>1.944643</td>\n",
       "      <td>1.021853</td>\n",
       "      <td>1.507937</td>\n",
       "      <td>0.450114</td>\n",
       "      <td>-1.191262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087010</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.045322</td>\n",
       "      <td>0.008147</td>\n",
       "      <td>0.024772</td>\n",
       "      <td>-0.037597</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>-0.007178</td>\n",
       "      <td>-0.009339</td>\n",
       "      <td>-0.007909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>42</td>\n",
       "      <td>1.944643</td>\n",
       "      <td>1.021853</td>\n",
       "      <td>1.507937</td>\n",
       "      <td>0.450114</td>\n",
       "      <td>-1.191262</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>-0.023429</td>\n",
       "      <td>0.021574</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>0.019832</td>\n",
       "      <td>-0.015388</td>\n",
       "      <td>-0.063456</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>-0.008983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>42</td>\n",
       "      <td>1.944643</td>\n",
       "      <td>1.021853</td>\n",
       "      <td>1.507937</td>\n",
       "      <td>0.450114</td>\n",
       "      <td>-1.191262</td>\n",
       "      <td>2</td>\n",
       "      <td>0.020688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113984</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>0.041785</td>\n",
       "      <td>-0.017249</td>\n",
       "      <td>-0.020224</td>\n",
       "      <td>-0.012810</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>-0.010003</td>\n",
       "      <td>-0.017164</td>\n",
       "      <td>0.010092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>42</td>\n",
       "      <td>1.944643</td>\n",
       "      <td>1.021853</td>\n",
       "      <td>1.507937</td>\n",
       "      <td>0.450114</td>\n",
       "      <td>-1.191262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125932</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>-0.027370</td>\n",
       "      <td>0.016466</td>\n",
       "      <td>-0.000992</td>\n",
       "      <td>-0.000881</td>\n",
       "      <td>-0.036223</td>\n",
       "      <td>-0.054404</td>\n",
       "      <td>-0.004215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>42</td>\n",
       "      <td>1.944643</td>\n",
       "      <td>1.021853</td>\n",
       "      <td>1.507937</td>\n",
       "      <td>0.450114</td>\n",
       "      <td>-1.191262</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040916</td>\n",
       "      <td>0.026587</td>\n",
       "      <td>0.028607</td>\n",
       "      <td>-0.008517</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.033174</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>-0.021701</td>\n",
       "      <td>-0.017020</td>\n",
       "      <td>0.001431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 521 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_y GENDER  AGE  OPENMINDEDNESS_Z  CONSCIENTIOUSNESS_Z  EXTRAVERSION_Z  \\\n",
       "0    25      F   42          1.944643             1.021853        1.507937   \n",
       "1    25      F   42          1.944643             1.021853        1.507937   \n",
       "2    25      F   42          1.944643             1.021853        1.507937   \n",
       "3    25      F   42          1.944643             1.021853        1.507937   \n",
       "4    25      F   42          1.944643             1.021853        1.507937   \n",
       "\n",
       "   AGREEABLENESS_Z  NEGATIVEEMOTIONALITY_Z  minutes         0  ...       502  \\\n",
       "0         0.450114               -1.191262        0  0.024715  ...  0.087010   \n",
       "1         0.450114               -1.191262        1  0.003987  ...  0.079375   \n",
       "2         0.450114               -1.191262        2  0.020688  ...  0.113984   \n",
       "3         0.450114               -1.191262        0  0.040507  ...  0.125932   \n",
       "4         0.450114               -1.191262        0  0.034863  ...  0.040916   \n",
       "\n",
       "        503       504       505       506       507       508       509  \\\n",
       "0  0.033386  0.045322  0.008147  0.024772 -0.037597  0.008539 -0.007178   \n",
       "1 -0.023429  0.021574  0.005550  0.019832 -0.015388 -0.063456  0.004368   \n",
       "2  0.014231  0.041785 -0.017249 -0.020224 -0.012810 -0.000855 -0.010003   \n",
       "3  0.006661  0.015620 -0.027370  0.016466 -0.000992 -0.000881 -0.036223   \n",
       "4  0.026587  0.028607 -0.008517  0.000826  0.033174 -0.004465 -0.021701   \n",
       "\n",
       "        510       511  \n",
       "0 -0.009339 -0.007909  \n",
       "1  0.016463 -0.008983  \n",
       "2 -0.017164  0.010092  \n",
       "3 -0.054404 -0.004215  \n",
       "4 -0.017020  0.001431  \n",
       "\n",
       "[5 rows x 521 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['OPENMINDEDNESS_Z', 'CONSCIENTIOUSNESS_Z', 'EXTRAVERSION_Z', 'AGREEABLENESS_Z', 'NEGATIVEEMOTIONALITY_Z']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating male, female, young and old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_train = data_train[data_train['GENDER'] == 'M']\n",
    "F_train = data_train[data_train['GENDER'] == 'F']\n",
    "\n",
    "M_val = data_validation[data_validation['GENDER'] == 'M']\n",
    "F_val = data_validation[data_validation['GENDER'] == 'F']\n",
    "\n",
    "M_test = data_test[data_test['GENDER'] == 'M']\n",
    "F_test = data_test[data_test['GENDER'] == 'F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = data_train[data_train['AGE'] <= 30]\n",
    "O_train = data_train[data_train['AGE'] > 30]\n",
    "\n",
    "Y_val = data_validation[data_validation['AGE'] <= 30]\n",
    "O_val = data_validation[data_validation['AGE'] > 30]\n",
    "\n",
    "Y_test = data_test[data_test['AGE'] <= 30]\n",
    "O_test = data_test[data_test['AGE'] > 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending validation data to train foro training with both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_train = M_train.append(M_val)\n",
    "F_train = F_train.append(F_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.append(Y_val)\n",
    "O_train = O_train.append(O_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_train.append(data_validation)\n",
    "df_validation = data_validation\n",
    "df_test = data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling\n",
    "\n",
    "Here we just pop out the labels from the main dataframe into separate dataframes for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined \n",
    "train_O = df_train.pop(labels[0]).to_numpy()\n",
    "train_C = df_train.pop(labels[1]).to_numpy()\n",
    "train_E = df_train.pop(labels[2]).to_numpy()\n",
    "train_A = df_train.pop(labels[3]).to_numpy()\n",
    "train_N = df_train.pop(labels[4]).to_numpy()\n",
    "\n",
    "val_O = df_validation.pop(labels[0]).to_numpy()\n",
    "val_C = df_validation.pop(labels[1]).to_numpy()\n",
    "val_E = df_validation.pop(labels[2]).to_numpy()\n",
    "val_A = df_validation.pop(labels[3]).to_numpy()\n",
    "val_N = df_validation.pop(labels[4]).to_numpy()\n",
    "\n",
    "# male train OCEAN \n",
    "M_train_O = M_train.pop(labels[0]).to_numpy()\n",
    "M_train_C = M_train.pop(labels[1]).to_numpy()\n",
    "M_train_E = M_train.pop(labels[2]).to_numpy()\n",
    "M_train_A = M_train.pop(labels[3]).to_numpy()\n",
    "M_train_N= M_train.pop(labels[4]).to_numpy()\n",
    "\n",
    "# female train OCEAN\n",
    "F_train_O = F_train.pop(labels[0]).to_numpy()\n",
    "F_train_C = F_train.pop(labels[1]).to_numpy()\n",
    "F_train_E = F_train.pop(labels[2]).to_numpy()\n",
    "F_train_A = F_train.pop(labels[3]).to_numpy()\n",
    "F_train_N= F_train.pop(labels[4]).to_numpy()\n",
    "\n",
    "# male validation OCEAN\n",
    "\n",
    "M_val_O = M_val.pop(labels[0]).to_numpy()\n",
    "M_val_C = M_val.pop(labels[1]).to_numpy()\n",
    "M_val_E = M_val.pop(labels[2]).to_numpy()\n",
    "M_val_A = M_val.pop(labels[3]).to_numpy()\n",
    "M_val_N = M_val.pop(labels[4]).to_numpy()\n",
    "\n",
    "# female validation OCEAN\n",
    "\n",
    "F_val_O = F_val.pop(labels[0]).to_numpy()\n",
    "F_val_C = F_val.pop(labels[1]).to_numpy()\n",
    "F_val_E = F_val.pop(labels[2]).to_numpy()\n",
    "F_val_A = F_val.pop(labels[3]).to_numpy()\n",
    "F_val_N = F_val.pop(labels[4]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# male train OCEAN \n",
    "Y_train_O = Y_train.pop(labels[0]).to_numpy()\n",
    "Y_train_C = Y_train.pop(labels[1]).to_numpy()\n",
    "Y_train_E = Y_train.pop(labels[2]).to_numpy()\n",
    "Y_train_A = Y_train.pop(labels[3]).to_numpy()\n",
    "Y_train_N=  Y_train.pop(labels[4]).to_numpy()\n",
    "\n",
    "# female train OCEAN\n",
    "O_train_O = O_train.pop(labels[0]).to_numpy()\n",
    "O_train_C = O_train.pop(labels[1]).to_numpy()\n",
    "O_train_E = O_train.pop(labels[2]).to_numpy()\n",
    "O_train_A = O_train.pop(labels[3]).to_numpy()\n",
    "O_train_N=  O_train.pop(labels[4]).to_numpy()\n",
    "\n",
    "# male validation OCEAN\n",
    "\n",
    "Y_val_O = Y_val.pop(labels[0]).to_numpy()\n",
    "Y_val_C = Y_val.pop(labels[1]).to_numpy()\n",
    "Y_val_E = Y_val.pop(labels[2]).to_numpy()\n",
    "Y_val_A = Y_val.pop(labels[3]).to_numpy()\n",
    "Y_val_N = Y_val.pop(labels[4]).to_numpy()\n",
    "\n",
    "# female validation OCEAN\n",
    "\n",
    "O_val_O = O_val.pop(labels[0]).to_numpy()\n",
    "O_val_C = O_val.pop(labels[1]).to_numpy()\n",
    "O_val_E = O_val.pop(labels[2]).to_numpy()\n",
    "O_val_A = O_val.pop(labels[3]).to_numpy()\n",
    "O_val_N = O_val.pop(labels[4]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we separate the data i.e only ID and minutes in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = ['ID_y', 'minutes']\n",
    "drop_cols = []\n",
    "\n",
    "train_data = df_train[data_cols]\n",
    "val_data   = df_validation[data_cols]\n",
    "test_data  = df_test[data_cols]\n",
    "\n",
    "Y_train_data = Y_train[data_cols]\n",
    "Y_val_data   = Y_val[data_cols]\n",
    "Y_test_data  = Y_test[data_cols]\n",
    "\n",
    "O_train_data = O_train[data_cols]\n",
    "O_val_data   = O_val[data_cols]\n",
    "O_test_data  = O_test[data_cols]\n",
    "\n",
    "M_train_data = M_train[data_cols]\n",
    "F_train_data = F_train[data_cols]\n",
    "M_val_data = M_val[data_cols]\n",
    "F_val_data = F_val[data_cols]\n",
    "M_test_data = M_test[data_cols]\n",
    "F_test_data = F_test[data_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "            ...\n",
      "            537, 538, 539, 540, 541, 542, 543, 544, 545, 546],\n",
      "           dtype='int64', length=691)\n",
      "Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "            ...\n",
      "            561, 562, 563, 564, 565, 566, 567, 568, 569, 570],\n",
      "           dtype='int64', length=579)\n"
     ]
    }
   ],
   "source": [
    "print(M_train_data.index)\n",
    "print(F_train_data.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popping the data columns, to leave only the BERT features to later convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = [ 'ID_y', 'GENDER', 'AGE', 'minutes']\n",
    "# combined\n",
    "train_com = df_train.drop(data_cols+drop_cols, axis = 1)\n",
    "val_com   = df_validation.drop(data_cols+drop_cols, axis = 1)\n",
    "test_com  = df_test.drop(data_cols+drop_cols, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# Age\n",
    "Y_train = Y_train.drop(data_cols+drop_cols, axis = 1)\n",
    "Y_val   = Y_val.drop(data_cols+drop_cols, axis = 1)\n",
    "Y_test  = Y_test.drop(data_cols+drop_cols, axis = 1)\n",
    "\n",
    "O_train = O_train.drop(data_cols+drop_cols, axis = 1)\n",
    "O_val   = O_val.drop(data_cols+drop_cols, axis = 1)\n",
    "O_test  = O_test.drop(data_cols+drop_cols, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# Gender\n",
    "M_train = M_train.drop(data_cols+drop_cols, axis = 1)\n",
    "M_val = M_val.drop(data_cols+drop_cols, axis = 1)\n",
    "M_test = M_test.drop(data_cols+drop_cols, axis = 1)\n",
    "\n",
    "F_train = F_train.drop(data_cols+drop_cols, axis = 1)\n",
    "F_val = F_val.drop(data_cols+drop_cols, axis = 1)\n",
    "F_test = F_test.drop(data_cols+drop_cols, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reseting indices to make it easier to combine later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comvined\n",
    "train_data.reset_index(drop = True, inplace = True)\n",
    "val_data.reset_index(drop = True, inplace = True)\n",
    "test_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "# Age\n",
    "\n",
    "Y_train.reset_index(drop=True, inplace=True)\n",
    "Y_train_data.reset_index(drop=True, inplace=True)\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "Y_test_data.reset_index(drop = True, inplace = True)\n",
    "Y_val.reset_index(drop=True, inplace=True)\n",
    "Y_val_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "O_train.reset_index(drop=True, inplace=True)\n",
    "O_train_data.reset_index(drop=True, inplace=True)\n",
    "O_val.reset_index(drop=True, inplace=True)\n",
    "O_val_data.reset_index(drop=True, inplace=True)\n",
    "O_test.reset_index(drop=True, inplace=True)\n",
    "O_test_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Gender\n",
    "M_train.reset_index(drop=True, inplace=True)\n",
    "M_train_data.reset_index(drop=True, inplace=True)\n",
    "M_test.reset_index(drop=True, inplace=True)\n",
    "M_test_data.reset_index(drop = True, inplace = True)\n",
    "M_val.reset_index(drop=True, inplace=True)\n",
    "M_val_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "F_train.reset_index(drop=True, inplace=True)\n",
    "F_train_data.reset_index(drop=True, inplace=True)\n",
    "F_val.reset_index(drop=True, inplace=True)\n",
    "F_val_data.reset_index(drop=True, inplace=True)\n",
    "F_test.reset_index(drop=True, inplace=True)\n",
    "F_test_data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.019520</td>\n",
       "      <td>-0.030385</td>\n",
       "      <td>-0.024921</td>\n",
       "      <td>-0.024894</td>\n",
       "      <td>0.035776</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>-0.029804</td>\n",
       "      <td>-0.008015</td>\n",
       "      <td>0.065737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087010</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.045322</td>\n",
       "      <td>0.008147</td>\n",
       "      <td>0.024772</td>\n",
       "      <td>-0.037597</td>\n",
       "      <td>0.008539</td>\n",
       "      <td>-0.007178</td>\n",
       "      <td>-0.009339</td>\n",
       "      <td>-0.007909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.044266</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>-0.025704</td>\n",
       "      <td>-0.010899</td>\n",
       "      <td>0.031770</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>-0.023154</td>\n",
       "      <td>0.019575</td>\n",
       "      <td>0.126032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>-0.023429</td>\n",
       "      <td>0.021574</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>0.019832</td>\n",
       "      <td>-0.015388</td>\n",
       "      <td>-0.063456</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>-0.008983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020688</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>-0.017838</td>\n",
       "      <td>-0.026574</td>\n",
       "      <td>-0.033762</td>\n",
       "      <td>0.026664</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>-0.020916</td>\n",
       "      <td>-0.017699</td>\n",
       "      <td>0.054905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113984</td>\n",
       "      <td>0.014231</td>\n",
       "      <td>0.041785</td>\n",
       "      <td>-0.017249</td>\n",
       "      <td>-0.020224</td>\n",
       "      <td>-0.012810</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>-0.010003</td>\n",
       "      <td>-0.017164</td>\n",
       "      <td>0.010092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040507</td>\n",
       "      <td>-0.006072</td>\n",
       "      <td>-0.032163</td>\n",
       "      <td>-0.033075</td>\n",
       "      <td>0.030305</td>\n",
       "      <td>0.010136</td>\n",
       "      <td>-0.004781</td>\n",
       "      <td>-0.037910</td>\n",
       "      <td>0.029789</td>\n",
       "      <td>0.067855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125932</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>-0.027370</td>\n",
       "      <td>0.016466</td>\n",
       "      <td>-0.000992</td>\n",
       "      <td>-0.000881</td>\n",
       "      <td>-0.036223</td>\n",
       "      <td>-0.054404</td>\n",
       "      <td>-0.004215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034863</td>\n",
       "      <td>-0.011329</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>-0.015245</td>\n",
       "      <td>0.036310</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>-0.009854</td>\n",
       "      <td>-0.056376</td>\n",
       "      <td>0.011678</td>\n",
       "      <td>0.048028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040916</td>\n",
       "      <td>0.026587</td>\n",
       "      <td>0.028607</td>\n",
       "      <td>-0.008517</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.033174</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>-0.021701</td>\n",
       "      <td>-0.017020</td>\n",
       "      <td>0.001431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.024715  0.019520 -0.030385 -0.024921 -0.024894  0.035776  0.006484   \n",
       "1  0.003987  0.044266  0.006456 -0.025704 -0.010899  0.031770  0.007720   \n",
       "2  0.020688  0.029968 -0.017838 -0.026574 -0.033762  0.026664 -0.012072   \n",
       "3  0.040507 -0.006072 -0.032163 -0.033075  0.030305  0.010136 -0.004781   \n",
       "4  0.034863 -0.011329 -0.000244 -0.015245  0.036310  0.036545 -0.009854   \n",
       "\n",
       "          7         8         9  ...       502       503       504       505  \\\n",
       "0 -0.029804 -0.008015  0.065737  ...  0.087010  0.033386  0.045322  0.008147   \n",
       "1 -0.023154  0.019575  0.126032  ...  0.079375 -0.023429  0.021574  0.005550   \n",
       "2 -0.020916 -0.017699  0.054905  ...  0.113984  0.014231  0.041785 -0.017249   \n",
       "3 -0.037910  0.029789  0.067855  ...  0.125932  0.006661  0.015620 -0.027370   \n",
       "4 -0.056376  0.011678  0.048028  ...  0.040916  0.026587  0.028607 -0.008517   \n",
       "\n",
       "        506       507       508       509       510       511  \n",
       "0  0.024772 -0.037597  0.008539 -0.007178 -0.009339 -0.007909  \n",
       "1  0.019832 -0.015388 -0.063456  0.004368  0.016463 -0.008983  \n",
       "2 -0.020224 -0.012810 -0.000855 -0.010003 -0.017164  0.010092  \n",
       "3  0.016466 -0.000992 -0.000881 -0.036223 -0.054404 -0.004215  \n",
       "4  0.000826  0.033174 -0.004465 -0.021701 -0.017020  0.001431  \n",
       "\n",
       "[5 rows x 512 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1270,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to numpy arrays to convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OCEAN_models = ['Model_O', 'Model_C', 'Model_E', 'Model_A', 'Model_N']\n",
    "# Combined\n",
    "train_com_np = np.array(train_com)\n",
    "val_com_np = np.array(val_com)\n",
    "test_com_np = np.array(test_com)\n",
    "\n",
    "# Age\n",
    "Y_train_np = np.array(Y_train)\n",
    "Y_test_np  = np.array(Y_test)\n",
    "Y_val_np   = np.array(Y_val)\n",
    "\n",
    "\n",
    "O_train_np = np.array(O_train)\n",
    "O_val_np   = np.array(O_val)\n",
    "O_test_np  = np.array(O_test)\n",
    "\n",
    "# GEnder\n",
    "M_train_np = np.array(M_train)\n",
    "M_test_np  = np.array(M_test)\n",
    "M_val_np   = np.array(M_val)\n",
    "\n",
    "\n",
    "F_train_np = np.array(F_train)\n",
    "F_val_np   = np.array(F_val)\n",
    "F_test_np  = np.array(F_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to tensor slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 19:22:21.967260: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-08 19:22:21.967291: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-08 19:22:21.967313: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vish-Lenovo-Yoga-S730-13IWL): /proc/driver/nvidia/version does not exist\n",
      "2021-12-08 19:22:21.967577: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Combined\n",
    "comb_O_train_set = tf.data.Dataset.from_tensor_slices((train_com_np, train_O))\n",
    "comb_C_train_set = tf.data.Dataset.from_tensor_slices((train_com_np, train_C))\n",
    "comb_E_train_set = tf.data.Dataset.from_tensor_slices((train_com_np, train_E))\n",
    "comb_A_train_set = tf.data.Dataset.from_tensor_slices((train_com_np, train_A))\n",
    "comb_N_train_set = tf.data.Dataset.from_tensor_slices((train_com_np, train_N))\n",
    "\n",
    "comb_O_validation_set = tf.data.Dataset.from_tensor_slices((val_com_np, val_O))\n",
    "comb_C_validation_set = tf.data.Dataset.from_tensor_slices((val_com_np, val_C))\n",
    "comb_E_validation_set = tf.data.Dataset.from_tensor_slices((val_com_np, val_E))\n",
    "comb_A_validation_set = tf.data.Dataset.from_tensor_slices((val_com_np, val_A))\n",
    "comb_N_validation_set = tf.data.Dataset.from_tensor_slices((val_com_np, val_N))\n",
    "\n",
    "\n",
    "# Age\n",
    "\n",
    "youngO_train_set      = tf.data.Dataset.from_tensor_slices((Y_train_np, Y_train_O))\n",
    "youngC_train_set      = tf.data.Dataset.from_tensor_slices((Y_train_np, Y_train_C))\n",
    "youngE_train_set      = tf.data.Dataset.from_tensor_slices((Y_train_np, Y_train_E))\n",
    "youngA_train_set      = tf.data.Dataset.from_tensor_slices((Y_train_np, Y_train_A))\n",
    "youngN_train_set      = tf.data.Dataset.from_tensor_slices((Y_train_np, Y_train_N))\n",
    "youngO_validation_set = tf.data.Dataset.from_tensor_slices((Y_val_np, Y_val_O))\n",
    "youngC_validation_set = tf.data.Dataset.from_tensor_slices((Y_val_np, Y_val_C))\n",
    "youngE_validation_set = tf.data.Dataset.from_tensor_slices((Y_val_np, Y_val_E))\n",
    "youngA_validation_set = tf.data.Dataset.from_tensor_slices((Y_val_np, Y_val_A))\n",
    "youngN_validation_set = tf.data.Dataset.from_tensor_slices((Y_val_np, Y_val_N))\n",
    "\n",
    "oldO_validation_set = tf.data.Dataset.from_tensor_slices((O_val_np, O_val_O))\n",
    "oldC_validation_set = tf.data.Dataset.from_tensor_slices((O_val_np, O_val_C))\n",
    "oldE_validation_set = tf.data.Dataset.from_tensor_slices((O_val_np, O_val_E))\n",
    "oldA_validation_set = tf.data.Dataset.from_tensor_slices((O_val_np, O_val_A))\n",
    "oldN_validation_set = tf.data.Dataset.from_tensor_slices((O_val_np, O_val_N))\n",
    "oldO_train_set      = tf.data.Dataset.from_tensor_slices((O_train_np, O_train_O))\n",
    "oldC_train_set      = tf.data.Dataset.from_tensor_slices((O_train_np, O_train_C))\n",
    "oldE_train_set      = tf.data.Dataset.from_tensor_slices((O_train_np, O_train_E))\n",
    "oldA_train_set      = tf.data.Dataset.from_tensor_slices((O_train_np, O_train_A))\n",
    "oldN_train_set      = tf.data.Dataset.from_tensor_slices((O_train_np, O_train_N))\n",
    "\n",
    "\n",
    "# Gender\n",
    "maleO_train_set = tf.data.Dataset.from_tensor_slices((M_train_np, M_train_O))\n",
    "maleC_train_set = tf.data.Dataset.from_tensor_slices((M_train_np, M_train_C))\n",
    "maleE_train_set = tf.data.Dataset.from_tensor_slices((M_train_np, M_train_E))\n",
    "maleA_train_set = tf.data.Dataset.from_tensor_slices((M_train_np, M_train_A))\n",
    "maleN_train_set = tf.data.Dataset.from_tensor_slices((M_train_np, M_train_N))\n",
    "\n",
    "femaleO_train_set = tf.data.Dataset.from_tensor_slices((F_train_np, F_train_O))\n",
    "femaleC_train_set = tf.data.Dataset.from_tensor_slices((F_train_np, F_train_C))\n",
    "femaleE_train_set = tf.data.Dataset.from_tensor_slices((F_train_np, F_train_E))\n",
    "femaleA_train_set = tf.data.Dataset.from_tensor_slices((F_train_np, F_train_A))\n",
    "femaleN_train_set = tf.data.Dataset.from_tensor_slices((F_train_np, F_train_N))\n",
    "\n",
    "maleO_validation_set = tf.data.Dataset.from_tensor_slices((M_val_np, M_val_O))\n",
    "maleC_validation_set = tf.data.Dataset.from_tensor_slices((M_val_np, M_val_C))\n",
    "maleE_validation_set = tf.data.Dataset.from_tensor_slices((M_val_np, M_val_E))\n",
    "maleA_validation_set = tf.data.Dataset.from_tensor_slices((M_val_np, M_val_A))\n",
    "maleN_validation_set = tf.data.Dataset.from_tensor_slices((M_val_np, M_val_N))\n",
    "\n",
    "femaleO_validation_set = tf.data.Dataset.from_tensor_slices((F_val_np, F_val_O))\n",
    "femaleC_validation_set = tf.data.Dataset.from_tensor_slices((F_val_np, F_val_C))\n",
    "femaleE_validation_set = tf.data.Dataset.from_tensor_slices((F_val_np, F_val_E))\n",
    "femaleA_validation_set = tf.data.Dataset.from_tensor_slices((F_val_np, F_val_A))\n",
    "femaleN_validation_set = tf.data.Dataset.from_tensor_slices((F_val_np, F_val_N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the train, validation and test sets for all to loop through while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined\n",
    "comb_train_set = [comb_O_train_set, \n",
    "                  comb_C_train_set, \n",
    "                  comb_E_train_set, \n",
    "                  comb_A_train_set, \n",
    "                  comb_N_train_set]\n",
    "\n",
    "comb_validation_set = [comb_O_validation_set, \n",
    "                       comb_C_validation_set, \n",
    "                       comb_E_validation_set, \n",
    "                       comb_A_validation_set, \n",
    "                       comb_N_validation_set]\n",
    "\n",
    "# Age\n",
    "young_train_set =     [youngO_train_set, \n",
    "                       youngC_train_set, \n",
    "                       youngE_train_set, \n",
    "                       youngA_train_set, \n",
    "                       youngN_train_set]\n",
    "young_validation_set =[youngO_validation_set, \n",
    "                       youngC_validation_set, \n",
    "                       youngE_validation_set, \n",
    "                       youngA_validation_set, \n",
    "                       youngN_validation_set]\n",
    "\n",
    "old_train_set =         [oldO_train_set, \n",
    "                         oldC_train_set, \n",
    "                         oldE_train_set,\n",
    "                         oldA_train_set,\n",
    "                         oldN_train_set]\n",
    "old_validation_set =    [oldO_validation_set,\n",
    "                         oldC_validation_set,\n",
    "                         oldE_validation_set,\n",
    "                         oldA_validation_set,\n",
    "                         oldN_validation_set]\n",
    "\n",
    "# Gender\n",
    "male_train_set =      [maleO_train_set, \n",
    "                       maleC_train_set, \n",
    "                       maleE_train_set, \n",
    "                       maleA_train_set, \n",
    "                       maleN_train_set]\n",
    "male_validation_set = [maleO_validation_set, \n",
    "                       maleC_validation_set, \n",
    "                       maleE_validation_set, \n",
    "                       maleA_validation_set, \n",
    "                       maleN_validation_set]\n",
    "\n",
    "female_train_set =      [femaleO_train_set, \n",
    "                         femaleC_train_set, \n",
    "                         femaleE_train_set,\n",
    "                         femaleA_train_set,\n",
    "                         femaleN_train_set]\n",
    "female_validation_set = [femaleO_validation_set,\n",
    "                         femaleC_validation_set,\n",
    "                         femaleE_validation_set,\n",
    "                         femaleA_validation_set,\n",
    "                         femaleN_validation_set]\n",
    "\n",
    "model_names = ['Model_O','Model_C','Model_E','Model_A','Model_N']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making folders to save models and dump training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = './Dump Data'\n",
    "path = './BERT Models'\n",
    "combined_path = './BERT Models/Combined'\n",
    "age_path = './BERT Models/Age'\n",
    "gender_path = './BERT Models/Gender'\n",
    "male_path = './BERT Models/Gender/Male'\n",
    "female_path = './BERT Models/Gender/Female'\n",
    "old_path = './BERT Models/Age/Old'\n",
    "young_path = './BERT Models/Age/Young'\n",
    "\n",
    "folders = [ dump\n",
    ",path\n",
    ",combined_path\n",
    ",age_path\n",
    ",gender_path\n",
    ",male_path\n",
    ",female_path\n",
    ",old_path\n",
    ",young_path]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: './Dump Data'\n",
      "[Errno 17] File exists: './BERT Models'\n",
      "[Errno 17] File exists: './BERT Models/Combined'\n",
      "[Errno 17] File exists: './BERT Models/Age'\n",
      "[Errno 17] File exists: './BERT Models/Gender'\n",
      "[Errno 17] File exists: './BERT Models/Gender/Male'\n",
      "[Errno 17] File exists: './BERT Models/Gender/Female'\n",
      "[Errno 17] File exists: './BERT Models/Age/Old'\n",
      "[Errno 17] File exists: './BERT Models/Age/Young'\n"
     ]
    }
   ],
   "source": [
    "for folder in folders:\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except OSError as error:\n",
    "        print(error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Train\n",
    "Currently:  \n",
    "Validation split: 0.15  \n",
    "Epochs: 1000  \n",
    "Trials: 100  \n",
    "## Combined models\n",
    "\"\"\"\n",
    "print(\"Combined TRAINING BEGINS\")\n",
    "for i in range(5):\n",
    "  train = comb_train_set[i]\n",
    "  val = comb_validation_set[i]\n",
    "  print(model_names[i])\n",
    "  # Define a regressor\n",
    "  total_reg = ak.StructuredDataRegressor(max_trials=100, overwrite=True,project_name = 'bert_combined_'+model_names[i], directory = './Dump Data')\n",
    "  # Feed the tensorflow Dataset to the regressor.\n",
    "  total_reg.fit(train, epochs=1000, validation_split=0.15)\n",
    "  # Convert to model   \n",
    "  total_model = total_reg.export_model()\n",
    "  # Evaluate on validation set\n",
    "  evaluation = total_reg.evaluate(val)\n",
    "  # Write loss and error to a file\n",
    "  with open('./Dump Data/bert_combined_eval_val.txt', 'a') as f:\n",
    "      f.write('bert_combined_'+model_names[i]+' -> ')\n",
    "      f.write(str(evaluation))\n",
    "      f.write('\\n')\n",
    "  # Save current model\n",
    "  total_model.save(combined_path+'/'+model_names[i])\n",
    "\n",
    "print(\"Combined training done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Train\n",
    "Currently:  \n",
    "Validation split: 0.15  \n",
    "Epochs: 1000  \n",
    "Trials: 100  \n",
    "## Age models\n",
    "\"\"\"\n",
    "print(\"Young TRAINING BEGINS\")\n",
    "for i in range(5):\n",
    "  train = young_train_set[i]\n",
    "  val = young_validation_set[i]\n",
    "  print(model_names[i])\n",
    "  # Define a regressor\n",
    "  total_reg = ak.StructuredDataRegressor(max_trials=100, overwrite=True,project_name = 'bert_young_'+model_names[i], directory = './Dump Data')\n",
    "  # Feed the tensorflow Dataset to the regressor.\n",
    "  total_reg.fit(train, epochs=1000, validation_split=0.15)\n",
    "  # Convert to model   \n",
    "  total_model = total_reg.export_model()\n",
    "  # Evaluate on validation set\n",
    "  evaluation = total_reg.evaluate(val)\n",
    "  # Write loss and error to a file\n",
    "  with open('./Dump Data/bert_young_eval_val.txt', 'a') as f:\n",
    "      f.write('bert_young_'+model_names[i]+' -> ')\n",
    "      f.write(str(evaluation))\n",
    "      f.write('\\n')\n",
    "  # Save current model\n",
    "  total_model.save(young_path+'/'+model_names[i])\n",
    "\n",
    "# \"\"\"## Old models\"\"\"\n",
    "print(\"Old TRAINING BEGINS\")\n",
    "\n",
    "for i in range(5):\n",
    "  train = old_train_set[i]\n",
    "  val = old_validation_set[i]\n",
    "  # Define a regressor\n",
    "  total_reg = ak.StructuredDataRegressor(max_trials=100, overwrite=True,project_name = 'bert_old_'+model_names[i], directory = './Dump Data')\n",
    "  # Feed the tensorflow Dataset to the regressor.\n",
    "  total_reg.fit(train, epochs=1000, validation_split=0.15)\n",
    "  # Convert to model   \n",
    "  total_model = total_reg.export_model()\n",
    "  # Evaluate on validation set\n",
    "  evaluation = total_reg.evaluate(val)\n",
    "  # Write loss and error to a file\n",
    "  with open('./Dump Data/bert_old_eval_val.txt', 'a') as f:\n",
    "      f.write('bert_old_'+model_names[i]+' -> ')\n",
    "      f.write(str(evaluation))\n",
    "      f.write('\\n')\n",
    "  # Save current model  \n",
    "  total_model.save(old_path+'/'+model_names[i])\n",
    "print(\"Age TRAINING DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Train\n",
    "Currently:  \n",
    "Validation split: 0.15  \n",
    "Epochs: 1000  \n",
    "Trials: 100  \n",
    "## Age models\n",
    "\"\"\"\n",
    "print(\"Male TRAINING BEGINS\")\n",
    "for i in range(5):\n",
    "  train = male_train_set[i]\n",
    "  val = male_validation_set[i]\n",
    "  print(model_names[i])\n",
    "  # Define a regressor\n",
    "  total_reg = ak.StructuredDataRegressor(max_trials=100, overwrite=True,project_name = 'bert_male_'+model_names[i], directory = './Dump Data')\n",
    "  # Feed the tensorflow Dataset to the regressor.\n",
    "  total_reg.fit(train, epochs=1000, validation_split=0.15)\n",
    "  # Convert to model   \n",
    "  total_model = total_reg.export_model()\n",
    "  # Evaluate on validation set\n",
    "  evaluation = total_reg.evaluate(val)\n",
    "  # Write loss and error to a file\n",
    "  with open('./Dump Data/bert_male_eval_val.txt', 'a') as f:\n",
    "      f.write('bert_male_'+model_names[i]+' -> ')\n",
    "      f.write(str(evaluation))\n",
    "      f.write('\\n')\n",
    "  # Save current model\n",
    "  total_model.save(male_path+'/'+model_names[i])\n",
    "\n",
    "# \"\"\"## Female models\"\"\"\n",
    "print(\"Female TRAINING BEGINS\")\n",
    "\n",
    "for i in range(5):\n",
    "  train = female_train_set[i]\n",
    "  val = female_validation_set[i]\n",
    "  # Define a regressor\n",
    "  total_reg = ak.StructuredDataRegressor(max_trials=100, overwrite=True,project_name = 'bert_female_'+model_names[i], directory = './Dump Data')\n",
    "  # Feed the tensorflow Dataset to the regressor.\n",
    "  total_reg.fit(train, epochs=1000, validation_split=0.15)\n",
    "  # Convert to model   \n",
    "  total_model = total_reg.export_model()\n",
    "  # Evaluate on validation set\n",
    "  evaluation = total_reg.evaluate(val)\n",
    "  # Write loss and error to a file\n",
    "  with open('./Dump Data/bert_female_eval_val.txt', 'a') as f:\n",
    "      f.write('bert_female_'+model_names[i]+' -> ')\n",
    "      f.write(str(evaluation))\n",
    "      f.write('\\n')\n",
    "  # Save current model  \n",
    "  total_model.save(female_path+'/'+model_names[i])\n",
    "print(\"Gender TRAINING DONE\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
